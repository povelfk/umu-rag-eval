{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Synthetic Data**\n",
    "\n",
    "In this notebook, we focus on **generating synthetic data** for training and evaluation purposes. This involves retrieving chunks of data, generating synthetic questions based on these chunks, and filtering and embedding the generated questions.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Retrieve Data Chunks:** Retrieve chunks of data from the search index using Azure Cognitive Search.\n",
    "2. **Generate Synthetic Questions:** Use a synthetic data generator to create questions based on the retrieved chunks.\n",
    "3. **Embed Questions:** Embed the filtered questions using a text embedding model.\n",
    "4. **Filter Questions:** Filter the generated questions using criteria such as Jaccard similarity and embedding similarity.\n",
    "5. **Store Data:** Save the high-quality synthetic questions to a JSON Lines file for future use.\n",
    "\n",
    "This notebook ensures that high-quality synthetic data is generated, filtered, and stored effectively, providing valuable data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.ai.projects import AIProjectClient\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "# # FOUNDRY\n",
    "# project_client = AIProjectClient(\n",
    "#     credential=DefaultAzureCredential(),\n",
    "#     endpoint=os.getenv(\"PROJECT_ENDPOINT\")\n",
    "# )\n",
    "# foundry_key = os.getenv(\"FOUNDRY_KEY\")\n",
    "\n",
    "# # Connetions:\n",
    "# search_connection = project_client.connections.get(\"searchpovel\")\n",
    "\n",
    "# AOAI\n",
    "chatModel = os.getenv(\"chatModel\")\n",
    "chatModelMini = os.getenv(\"chatModelMini\")\n",
    "chatModelMiniFast = os.getenv(\"chatModelMiniFast\")\n",
    "aoai_embedding_model = os.getenv(\"embeddingModel\")\n",
    "\n",
    "aoai_version = os.getenv(\"AOAI_API_VERSION\")\n",
    "aoai_endpoint = os.getenv(\"AOAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"AOAI_KEY\")\n",
    "\n",
    "# SEARCH\n",
    "search_key = os.getenv(\"SEARCH_KEY\")\n",
    "search_credential = AzureKeyCredential(search_key)\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\") #search_connection.target\n",
    "index_name = os.getenv(\"SEARCH_INDEX_NAME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 Retrieve Data Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.search import get_all_chunks\n",
    "\n",
    "# Retrieve all chunks from the search index\n",
    "chunks = get_all_chunks(\n",
    "    index=index_name,\n",
    "    credential=search_credential,\n",
    "    search_endpoint=search_endpoint,\n",
    "    k=200  # Adjust k as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Generate synthetic samples**\n",
    "\n",
    "**Generation Strategy:**\n",
    "- For each chunk, find k=5 most similar chunks using cosine similarity (k-NN)\n",
    "- Generate questions focused on the main chunk's topic\n",
    "- A question can either be grounded, meaning that the correct answer can be found in the provided chunk(s), or not grounded, meaning that it cannot be answered by the provided chunk(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generation Settings**\n",
    "\n",
    "The synthetic questions are generated with various characteristics to ensure diverse, realistic test data. \n",
    "\n",
    "**Configuration Files:** All settings are stored in `configs/settings/` and can be customized:\n",
    "\n",
    "- **`domains.jsonl`** - Question domains (e.g., \"Related to Product\", \"Related to Customer\")\n",
    "- **`difficulties.jsonl`** - Complexity levels (Beginner, Intermediate, Advanced, Expert)\n",
    "- **`tones.jsonl`** - Question styles (Neutral, Friendly, Angry)\n",
    "- **`languages.jsonl`** - Language options (English, Swedish)\n",
    "- **`length_categories.jsonl`** - Question lengths (Short 5-7 words, Medium 8-15 words, Long 16-25 words, Very Long 25+ words)\n",
    "- **`topics.jsonl`** - Topic selection strategy\n",
    "\n",
    "Each option has a **weight** that controls how often it's selected during generation. Higher weights = more frequently used.\n",
    "\n",
    "**Run the cell below to see the current configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " SYNTHETIC QUESTION GENERATION SETTINGS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ Domains:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ Related to Product             Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Related to Customer            Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ðŸ“‹ Difficulties:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ Beginner                       Weight: 0.25  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Intermediate                   Weight: 0.30  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Advanced                       Weight: 0.25  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Expert                         Weight: 0.20  â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ðŸ“‹ Tones:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ Neutral                        Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Friendly                       Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Angry                          Weight: 0.50  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ðŸ“‹ Languages:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ English                        Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Swedish                        Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ðŸ“‹ Question Lengths:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ Short (5-7 words)              Weight: 0.30  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Medium (8-15 words)            Weight: 0.40  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Long (16-25 words)             Weight: 0.20  â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  â€¢ Very Long (25+ words)          Weight: 0.10  â–ˆâ–ˆ\n",
      "\n",
      "ðŸ“‹ Topics:\n",
      "----------------------------------------------------------------------\n",
      "  â€¢ Same as the golden question    Weight: 1.00  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¡ Higher weights = more likely to be selected during generation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from utils.helper import load_and_display_settings\n",
    "load_and_display_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import SyntheticDataGenerator, generate_synthetic_questions\n",
    "\n",
    "# Instantiate your generator\n",
    "sdg_generator = SyntheticDataGenerator(chatModel)\n",
    "\n",
    "# Generate synthetic questions using k-NN for similar chunks\n",
    "synthetic_data, failed_samples = generate_synthetic_questions(\n",
    "    chunks,\n",
    "    sdg_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of successful samples: {len(synthetic_data)}\")\n",
    "print(f\"number of failed samples: {len(failed_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 Embed synthetic question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm import embed_text\n",
    "\n",
    "def embed_single_item(item):\n",
    "    \"\"\"Helper function to embed a single item\"\"\"\n",
    "    item[\"synthetic_question_embedding\"] = embed_text(item[\"synthetic_question\"])\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Count total questions\n",
    "total_questions = len(synthetic_data)\n",
    "\n",
    "# Parallel embedding with progress bar\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for item in synthetic_data:\n",
    "        future = executor.submit(embed_single_item, item)\n",
    "        futures.append(future)\n",
    "    \n",
    "    with tqdm(total=total_questions, desc=\"Embedding all questions\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # This updates the item in-place\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 Filter out low quality samples**\n",
    "- Remove duplicates, too short, and too long questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clean_data import filter_synthetic_questions\n",
    "\n",
    "accepted_samples, rejected_samples = filter_synthetic_questions(\n",
    "    synthetic_data=synthetic_data,\n",
    "    min_question_length=5,\n",
    "    max_question_length=150,\n",
    "    similarity_threshold=0.95, # Adjust as needed\n",
    "    remove_duplicates=True # Set to False to skip duplicate detection\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Processing complete!\")\n",
    "print(f\"Accepted samples: {len(accepted_samples)}\")\n",
    "print(f\"Rejected samples: {len(rejected_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 Storing synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def save_data(json_list, file_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    # Serialize list fields to JSON strings for CSV storage\n",
    "    for item in json_list:\n",
    "        if 'similar_chunks' in item and isinstance(item['similar_chunks'], list):\n",
    "            item['similar_chunks'] = json.dumps(item['similar_chunks'])\n",
    "    \n",
    "    df = pd.json_normalize(json_list)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(accepted_samples, 'data/synthetic_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
