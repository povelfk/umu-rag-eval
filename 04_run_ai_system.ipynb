{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058eb49d",
   "metadata": {},
   "source": [
    "# **Run Completion System**\n",
    "\n",
    "Generate responses from the Completion system for synthetic test questions. This notebook evaluates the **generation component** by providing fixed context (no retrieval) to test the LLM's ability to answer questions accurately.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Load Test Data:** Read synthetic questions with their associated chunks\n",
    "2. **Define Completion System:** Set up the LLM with instructions for answering from context\n",
    "3. **Generate Responses:** Run all questions through the system with fixed context (main + 5 similar chunks)\n",
    "4. **Save Results:** Store responses for groundedness evaluation\n",
    "\n",
    "**Important:** This uses hardcoded chunks from generation (no retrieval), isolating LLM performance from search quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a079dd",
   "metadata": {},
   "source": [
    "## **Load Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69d37b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 172\n",
      "Grounded: 88\n",
      "Not-grounded: 84\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synthetic_question</th>\n",
       "      <th>explanation</th>\n",
       "      <th>synthetic_response</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>synthetic_chunk_id</th>\n",
       "      <th>is_grounded</th>\n",
       "      <th>main_chunk</th>\n",
       "      <th>similar_chunks</th>\n",
       "      <th>domain</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>tone</th>\n",
       "      <th>language</th>\n",
       "      <th>question_length</th>\n",
       "      <th>synthetic_question_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does CompactCook Camping Stove come with a man...</td>\n",
       "      <td>The provided context details product features,...</td>\n",
       "      <td>I don't have information about a manufacturer ...</td>\n",
       "      <td>c34dafd3cd3a_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvY...</td>\n",
       "      <td>c34dafd3cd3a_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvY...</td>\n",
       "      <td>False</td>\n",
       "      <td># Information about product item_number: 20\\nC...</td>\n",
       "      <td>[# Information about product item_number: 13\\n...</td>\n",
       "      <td>Related to Customer</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>English</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.025468595325946808, -0.01461399719119072, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  synthetic_question  \\\n",
       "0  Does CompactCook Camping Stove come with a man...   \n",
       "\n",
       "                                         explanation  \\\n",
       "0  The provided context details product features,...   \n",
       "\n",
       "                                  synthetic_response  \\\n",
       "0  I don't have information about a manufacturer ...   \n",
       "\n",
       "                                            chunk_id  \\\n",
       "0  c34dafd3cd3a_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvY...   \n",
       "\n",
       "                                  synthetic_chunk_id  is_grounded  \\\n",
       "0  c34dafd3cd3a_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvY...        False   \n",
       "\n",
       "                                          main_chunk  \\\n",
       "0  # Information about product item_number: 20\\nC...   \n",
       "\n",
       "                                      similar_chunks               domain  \\\n",
       "0  [# Information about product item_number: 13\\n...  Related to Customer   \n",
       "\n",
       "     difficulty     tone language  question_length  \\\n",
       "0  Intermediate  Neutral  English               11   \n",
       "\n",
       "                        synthetic_question_embedding  \n",
       "0  [-0.025468595325946808, -0.01461399719119072, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dataset=\"synthetic_samples\"\n",
    "\n",
    "fpath=f'data/{dataset}.csv'\n",
    "data = pd.read_csv(fpath)\n",
    "\n",
    "# Deserialize similar_chunks from JSON string\n",
    "data['similar_chunks'] = data['similar_chunks'].apply(json.loads)\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Grounded: {data['is_grounded'].sum()}\")\n",
    "print(f\"Not-grounded: {(~data['is_grounded']).sum()}\")\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94641334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "# AOAI\n",
    "chatModel = \"gpt-4o-mini\" # os.getenv(\"chatModelMini\")\n",
    "aoai_version = os.getenv(\"AOAI_API_VERSION\")\n",
    "aoai_endpoint = os.getenv(\"AOAI_ENDPOINT\")\n",
    "aoai_key = os.getenv(\"FOUNDRY_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbda5ea",
   "metadata": {},
   "source": [
    "## **Define Completion System**\n",
    "\n",
    "**Important:** This uses the exact chunks from generation (main + similar chunks).  \n",
    "No retrieval is performed - we test the LLM's ability to work with fixed context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa4769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm import invoke_llm\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- If the context contains information relevant to the question, provide a clear and accurate answer based ONLY on that context.\n",
    "- If the context does NOT contain enough information to answer the question, respond with: \"I don't have enough information in the provided context to answer this question.\"\n",
    "- Do not make up information or use knowledge outside the provided context.\n",
    "- Be concise and direct in your responses.\n",
    "- Always respond in the same language as the most recent question.\"\"\"\n",
    "\n",
    "def generate_response(row, chatModel):\n",
    "    question = row['synthetic_question']\n",
    "    main_chunk = row['main_chunk']\n",
    "    similar_chunks = row['similar_chunks']  # Already deserialized as list\n",
    "    \n",
    "    # Reconstruct full context (main + similar chunks)\n",
    "    context_parts = [f\"MAIN CONTEXT:\\n{main_chunk}\"]\n",
    "    for i, chunk in enumerate(similar_chunks, 1):\n",
    "        context_parts.append(f\"ADDITIONAL CONTEXT {i}:\\n{chunk}\")\n",
    "    \n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create user message with context and question\n",
    "    user_message = f\"\"\"\n",
    "    Context:\n",
    "    {full_context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = invoke_llm(\n",
    "            system=SYSTEM_PROMPT,\n",
    "            user=user_message,\n",
    "            model=chatModel\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating response: {e}\")\n",
    "        return f\"ERROR: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff854b",
   "metadata": {},
   "source": [
    "### **Generate Responses**\n",
    "Parallel processing with 10 workers to generate completion responses for all test questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d3f04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating completion responses for 172 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating completion responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [01:07<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All responses generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the new column\n",
    "data['completion_response'] = None\n",
    "\n",
    "print(f\"üöÄ Generating completion responses for {len(data)} questions...\")\n",
    "\n",
    "# Parallel processing with progress bar\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for idx, row in data.iterrows():\n",
    "        future = executor.submit(generate_response, row, chatModel)\n",
    "        futures.append((future, idx))\n",
    "    \n",
    "    with tqdm(total=len(data), desc=\"Generating completion responses\") as pbar:\n",
    "        for future, idx in futures:\n",
    "            try:\n",
    "                response = future.result()\n",
    "                data.at[idx, 'completion_response'] = response\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error at index {idx}: {e}\")\n",
    "                data.at[idx, 'completion_response'] = f\"ERROR: {str(e)}\"\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"‚úÖ All responses generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7e6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Response Statistics:\n",
      "   Total responses: 172\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis of generated responses\n",
    "print(\"\\nüìä Response Statistics:\")\n",
    "print(f\"   Total responses: {len(data)}\")\n",
    "print(f\"   Errors: {data['completion_response'].str.startswith('ERROR').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efac29f",
   "metadata": {},
   "source": [
    "## **Save Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efac29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved results to: data/synthetic_samples_with_completion_responses_gpt-4o-mini.csv\n",
      "\n",
      "üìã Final columns: ['synthetic_question', 'explanation', 'synthetic_response', 'chunk_id', 'synthetic_chunk_id', 'is_grounded', 'main_chunk', 'similar_chunks', 'domain', 'difficulty', 'tone', 'language', 'question_length', 'synthetic_question_embedding', 'completion_response']\n"
     ]
    }
   ],
   "source": [
    "# Save results to new CSV\n",
    "# Re-serialize similar_chunks to JSON before saving\n",
    "data['similar_chunks'] = data['similar_chunks'].apply(json.dumps)\n",
    "\n",
    "output_path = f'data/{dataset}_with_completion_responses_{chatModel}.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Saved results to: {output_path}\")\n",
    "print(f\"\\nüìã Final columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be806d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e6dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980d016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965842f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
